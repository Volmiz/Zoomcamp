{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7122b9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the dataset\n",
    "data = pd.read_csv(\"C:\\Data\\data.homework_3.csv\")\n",
    "\n",
    "# Select relevant columns\n",
    "data = data[['Make', 'Model', 'Year', 'Engine HP', 'Engine Cylinders', 'Transmission Type', 'Vehicle Style', 'highway MPG', 'city mpg', 'MSRP']]\n",
    "\n",
    "# Lowercase column names and replace spaces with underscores\n",
    "data.columns = data.columns.str.lower().str.replace(' ', '_')\n",
    "\n",
    "# Fill missing values with 0\n",
    "data = data.fillna(0)\n",
    "\n",
    "# Create binary target variable 'above_average'\n",
    "data['above_average'] = (data['msrp'] > data['msrp'].mean()).astype(int)\n",
    "\n",
    "# Split the data into train/validation/test sets\n",
    "df_full_train, df_test = train_test_split(data, test_size=0.2, random_state=1)\n",
    "df_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=1)\n",
    "\n",
    "# Define numerical and categorical columns\n",
    "numerical = ['engine_hp', 'engine_cylinders', 'highway_mpg', 'city_mpg']\n",
    "categorical = ['make', 'model', 'year', 'transmission_type', 'vehicle_style']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a858ef",
   "metadata": {},
   "source": [
    "Question 1: ROC AUC feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3c3fbdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer for Question 1: engine_hp\n"
     ]
    }
   ],
   "source": [
    "def calculate_auc(df_train, column):\n",
    "    auc = roc_auc_score(df_train['above_average'], df_train[column])\n",
    "    if auc < 0.5:\n",
    "        auc = roc_auc_score(df_train['above_average'], -df_train[column])\n",
    "    return auc\n",
    "\n",
    "auc_scores = {column: calculate_auc(df_train, column) for column in numerical}\n",
    "best_numerical_variable = max(auc_scores, key=auc_scores.get)\n",
    "print(f'Answer for Question 1: {best_numerical_variable}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafd2f5f",
   "metadata": {},
   "source": [
    "Question 2: Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3ac556c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer for Question 2: 0.980\n"
     ]
    }
   ],
   "source": [
    "columns = categorical + numerical\n",
    "train_dicts = df_train[columns].to_dict(orient='records')\n",
    "dv = DictVectorizer(sparse=False)\n",
    "X_train = dv.fit_transform(train_dicts)\n",
    "model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000)\n",
    "model.fit(X_train, df_train['above_average'])\n",
    "\n",
    "# Evaluate AUC on the validation dataset\n",
    "val_dicts = df_val[columns].to_dict(orient='records')\n",
    "X_val = dv.transform(val_dicts)\n",
    "y_pred = model.predict_proba(X_val)[:, 1]\n",
    "val_auc = roc_auc_score(df_val['above_average'], y_pred)\n",
    "print(f'Answer for Question 2: {val_auc:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98a5542",
   "metadata": {},
   "source": [
    "Question 3: Precision and Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a1691cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer for Question 3: 0.49\n"
     ]
    }
   ],
   "source": [
    "thresholds = np.arange(0, 1.01, 0.01)\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_bin = (y_pred >= threshold).astype(int)\n",
    "    tp = ((y_pred_bin == 1) & (df_val['above_average'] == 1)).sum()\n",
    "    fp = ((y_pred_bin == 1) & (df_val['above_average'] == 0)).sum()\n",
    "    fn = ((y_pred_bin == 0) & (df_val['above_average'] == 1)).sum()\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "\n",
    "# Find the threshold where precision and recall intersect\n",
    "intersection = np.argwhere(np.diff(np.sign(np.array(precision_scores) - np.array(recall_scores))) != 0).reshape(-1) + 1\n",
    "optimal_threshold = thresholds[intersection[0]]\n",
    "print(f'Answer for Question 3: {optimal_threshold:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5388221",
   "metadata": {},
   "source": [
    "Question 4: F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ae4d597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer for Question 4: 0.51\n"
     ]
    }
   ],
   "source": [
    "f1_scores = [2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0 for precision, recall in zip(precision_scores, recall_scores)]\n",
    "max_f1_score = max(f1_scores)\n",
    "optimal_threshold_f1 = thresholds[f1_scores.index(max_f1_score)]\n",
    "print(f'Answer for Question 4: {optimal_threshold_f1:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0ee91d",
   "metadata": {},
   "source": [
    "Question 5: 5-Fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a31e92c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer for Question 5: 0.002\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "scores = []\n",
    "\n",
    "for train_idx, val_idx in kfold.split(df_full_train):\n",
    "    df_train = df_full_train.iloc[train_idx]\n",
    "    df_val = df_full_train.iloc[val_idx]\n",
    "    \n",
    "    dv = DictVectorizer(sparse=False)\n",
    "    X_train = dv.fit_transform(df_train[columns].to_dict(orient='records'))\n",
    "    model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000)\n",
    "    model.fit(X_train, df_train['above_average'])\n",
    "    \n",
    "    val_dicts = df_val[columns].to_dict(orient='records')\n",
    "    X_val = dv.transform(val_dicts)\n",
    "    y_pred = model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    auc = roc_auc_score(df_val['above_average'], y_pred)\n",
    "    scores.append(auc)\n",
    "\n",
    "std_deviation = round(np.std(scores), 3)\n",
    "print(f'Answer for Question 5: {std_deviation}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ba65fb",
   "metadata": {},
   "source": [
    "Question 6: Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ffb0bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer for Question 6: 10\n"
     ]
    }
   ],
   "source": [
    "C_values = [0.01, 0.1, 0.5, 10]\n",
    "mean_scores = []\n",
    "std_scores = []\n",
    "\n",
    "for C in C_values:\n",
    "    kfold = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    scores = []\n",
    "    \n",
    "    for train_idx, val_idx in kfold.split(df_full_train):\n",
    "        df_train = df_full_train.iloc[train_idx]\n",
    "        df_val = df_full_train.iloc[val_idx]\n",
    "        \n",
    "        dv = DictVectorizer(sparse=False)\n",
    "        X_train = dv.fit_transform(df_train[columns].to_dict(orient='records'))\n",
    "        model = LogisticRegression(solver='liblinear', C=C, max_iter=1000)\n",
    "        model.fit(X_train, df_train['above_average'])\n",
    "        \n",
    "        val_dicts = df_val[columns].to_dict(orient='records')\n",
    "        X_val = dv.transform(val_dicts)\n",
    "        y_pred = model.predict_proba(X_val)[:, 1]\n",
    "        \n",
    "        auc = roc_auc_score(df_val['above_average'], y_pred)\n",
    "        scores.append(auc)\n",
    "    \n",
    "    mean_auc = round(np.mean(scores), 3)\n",
    "    std_auc = round(np.std(scores), 3)\n",
    "    \n",
    "    mean_scores.append(mean_auc)\n",
    "    std_scores.append(std_auc)\n",
    "\n",
    "best_mean_score = max(mean_scores)\n",
    "best_std_score = std_scores[mean_scores.index(best_mean_score)]\n",
    "best_C = C_values[mean_scores.index(best_mean_score)]\n",
    "print(f'Answer for Question 6: {best_C}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
